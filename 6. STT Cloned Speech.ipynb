{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9daccbc5",
   "metadata": {},
   "source": [
    "# STT Cloned Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36cd3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torch.nn.functional as F\n",
    "from audioseal import AudioSeal\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38d00967",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_path = \"../Dataset/Watermarked Audio\"\n",
    "unwatermarked_path = \"../Dataset/Unwatermarked Audio\"\n",
    "transcription_path = '../Dataset/Transcriptions/transcriptions_complete.csv'\n",
    "results_path = '../Dataset/Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4012ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the watermark detector\n",
    "detector = AudioSeal.load_detector(\"audioseal_detector_16bits\")\n",
    "\n",
    "# 2. Import the model for transcription\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device)\n",
    "\n",
    "# Whisper and AudioSeal expect a sample rate of 16khz\n",
    "target_sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "297dfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filepaths\n",
    "unwatermarked_files = os.listdir(unwatermarked_path)\n",
    "unwatermarked_files = [i for i in unwatermarked_files if i[-4:] == \".mp3\"]\n",
    "unwatermarked_files = [i for i in unwatermarked_files if \"audioseal\" in i]\n",
    "\n",
    "watermarked_files = os.listdir(watermarked_path)\n",
    "watermarked_files = [i for i in watermarked_files if i[-4:] == \".mp3\"]\n",
    "watermarked_files = [i for i in watermarked_files if \"audioseal\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e7d4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to extract the ID from the filename\n",
    "def extract_id(filename):\n",
    "    match = re.search(r'common_voice_en_(\\d+)', filename)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "206bbc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audios to evaluate: 14,124\n"
     ]
    }
   ],
   "source": [
    "# Build dicts by ID\n",
    "un_dict = {extract_id(f): f for f in unwatermarked_files}\n",
    "w_dict = {extract_id(f): f for f in watermarked_files}\n",
    "\n",
    "# Find common IDs and build a dict with (un, w) tuples\n",
    "matched = {id_: (un_dict[id_], w_dict[id_]) for id_ in un_dict.keys() & w_dict.keys()}\n",
    "\n",
    "print(f\"Audios to evaluate: {len(matched):0,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8962b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining clips 0 (0.0%)\n",
      "Last file number:  36\n"
     ]
    }
   ],
   "source": [
    "# Transcriptios of the original file\n",
    "transcriptions = pd.read_csv('../Dataset/Transcriptions/transcriptions_complete.csv')\n",
    "# Create ID\n",
    "transcriptions[\"id\"] = transcriptions[\"clip\"].apply(extract_id)\n",
    "\n",
    "# Find IDs already processed\n",
    "processed_ids = []\n",
    "last_file_num = 0\n",
    "for file in os.listdir(results_path):\n",
    "    file_path = os.path.join(results_path, file)\n",
    "    if file.startswith(\"results_transcription_\"):\n",
    "        file_num = re.search(r'results_transcription_(\\d+).csv', file).group(1)\n",
    "        last_file_num = np.max([int(file_num), last_file_num])\n",
    "        temp = pd.read_csv(file_path, usecols=[\"id\"])\n",
    "        processed_ids.extend(temp[\"id\"].astype(str).tolist())\n",
    "\n",
    "remaining_clips = list(matched.keys() - set(processed_ids))\n",
    "print(f\"Remaining clips {len(remaining_clips):0,.0f} ({len(remaining_clips)/len(matched.keys()):0.1%})\")\n",
    "print(\"Last file number: \", last_file_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "822c94f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/846 [00:00<?, ?it/s]/u6/lgomezto/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_scores` is. When `return_dict_in_generate` is not `True`, `output_scores` is ignored.\n",
      "  warnings.warn(\n",
      "100%|██████████| 846/846 [15:19<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "batch_size = 2**4  # Adjust based on your GPU memory\n",
    "batch_num = 0\n",
    "\n",
    "# Store results incrementally\n",
    "results_batch = []     # Stores intermediate batch results\n",
    "\n",
    "# Iterate over remaining clips in batches\n",
    "for _ in tqdm(range(0, len(remaining_clips), batch_size)):\n",
    "    batch_ids = remaining_clips[:batch_size]  # Take a batch of clip IDs\n",
    "    batch_audio = []\n",
    "    watermark_probs = []\n",
    "\n",
    "    for id_ in batch_ids:\n",
    "        un, w = matched[id_]\n",
    "        clip_path = os.path.join(unwatermarked_path, un)\n",
    "\n",
    "        # Load audio using torchaudio for Whisper compatibility\n",
    "        wav, sr = torchaudio.load(clip_path)\n",
    "\n",
    "        # Convert to mono if needed\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample to target sampling rate\n",
    "        wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n",
    "\n",
    "        # --- Step 2: Add audio to batch for transcription ---\n",
    "        batch_audio.append(wav)\n",
    "\n",
    "    # --- Step 3: Pad audio to equal length for batch processing ---\n",
    "    max_len = max(wav.shape[1] for wav in batch_audio)\n",
    "    batch_padded = [F.pad(wav, (0, max_len - wav.shape[1])) for wav in batch_audio]\n",
    "    batch_tensor = torch.stack(batch_padded).squeeze(1)  # (batch, time)\n",
    "\n",
    "    # --- Step 4: Convert to Whisper input format (list of numpy arrays) ---\n",
    "    batch_np = [wav.cpu().numpy() for wav in batch_tensor]\n",
    "    input_features = processor.feature_extractor(\n",
    "        batch_np, sampling_rate=target_sr, return_tensors=\"pt\"\n",
    "    ).input_features.to(device, dtype=torch.float16)\n",
    "\n",
    "    # --- Step 5: Generate transcriptions with Whisper ---\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_features,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded = processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "    # --- Step 6: Compute confidence scores ---\n",
    "    scores = outputs.scores\n",
    "    if scores:\n",
    "        token_probs = [F.softmax(logits, dim=-1).max() for logits in scores]\n",
    "        confidences = [torch.mean(p).item() for p in token_probs]\n",
    "    else:\n",
    "        confidences = [None] * len(batch_ids)\n",
    "\n",
    "    # --- Step 7: Store results in memory ---\n",
    "    for id_, transcription, confidence, in zip(batch_ids, decoded, confidences):\n",
    "        results_batch.append({\n",
    "            \"id\": id_,\n",
    "            \"unwatermarked_transcription\": transcription,\n",
    "            \"unwatermarked_confidence\": confidence,\n",
    "        })\n",
    "\n",
    "    # --- Step 8: Update progress ---\n",
    "    batch_num += 1\n",
    "    remaining_clips = remaining_clips[batch_size:]  # Remove processed clips\n",
    "\n",
    "    # --- Step 9: Save batch results every N batches or at the end ---\n",
    "    if (batch_num % 25 == 0) or (len(remaining_clips) == 0):\n",
    "        last_file_num += 1\n",
    "        batch_df = pd.DataFrame(results_batch)\n",
    "        output_filename = f\"results_transcription_{last_file_num}.csv\"\n",
    "        batch_df.to_csv(os.path.join(results_path, output_filename), index=False)\n",
    "        results_batch = []  # Reset batch storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb47b042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14124, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = pd.DataFrame()\n",
    "for file in os.listdir(results_path):\n",
    "    file_path = os.path.join(results_path, file)\n",
    "    if file.startswith(\"results_transcription_\"):\n",
    "        temp = pd.read_csv(file_path)\n",
    "        all_results = pd.concat([all_results, temp])\n",
    "all_results = all_results.drop_duplicates(subset=[\"id\"])\n",
    "all_results = all_results.reset_index(drop=True)\n",
    "all_results.to_csv(os.path.join(results_path, \"all_results_transcription.csv\"), index=False)\n",
    "all_results.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
