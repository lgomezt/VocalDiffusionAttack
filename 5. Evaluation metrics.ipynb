{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9daccbc5",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd840bbc",
   "metadata": {},
   "source": [
    "For each cloned audio file:\n",
    "1.\tCheck whether the watermark is still present.\n",
    "2.\tAssess whether the content remains the same.\n",
    "3.\tEvaluate any changes in audio quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "36cd3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torch.nn.functional as F\n",
    "from audioseal import AudioSeal\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "38d00967",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_path = \"../Dataset/Watermarked Audio\"\n",
    "unwatermarked_path = \"../Dataset/Unwatermarked Audio\"\n",
    "transcription_path = '../Dataset/Transcriptions/transcriptions_complete.csv'\n",
    "results_path = '../Dataset/Results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e4012ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the watermark detector\n",
    "detector = AudioSeal.load_detector(\"audioseal_detector_16bits\")\n",
    "\n",
    "# 2. Import the model for transcription\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device)\n",
    "\n",
    "# Whisper and AudioSeal expect a sample rate of 16khz\n",
    "target_sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "297dfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filepaths\n",
    "unwatermarked_files = os.listdir(unwatermarked_path)\n",
    "unwatermarked_files = [i for i in unwatermarked_files if i[-4:] == \".mp3\"]\n",
    "unwatermarked_files = [i for i in unwatermarked_files if \"audioseal\" in i]\n",
    "\n",
    "watermarked_files = os.listdir(watermarked_path)\n",
    "watermarked_files = [i for i in watermarked_files if i[-4:] == \".mp3\"]\n",
    "watermarked_files = [i for i in watermarked_files if \"audioseal\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e7d4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to extract the ID from the filename\n",
    "def extract_id(filename):\n",
    "    match = re.search(r'common_voice_en_(\\d+)', filename)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "206bbc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audios to evaluate: 14,124\n"
     ]
    }
   ],
   "source": [
    "# Build dicts by ID\n",
    "un_dict = {extract_id(f): f for f in unwatermarked_files}\n",
    "w_dict = {extract_id(f): f for f in watermarked_files}\n",
    "\n",
    "# Find common IDs and build a dict with (un, w) tuples\n",
    "matched = {id_: (un_dict[id_], w_dict[id_]) for id_ in un_dict.keys() & w_dict.keys()}\n",
    "\n",
    "print(f\"Audios to evaluate: {len(matched):0,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcriptios of the original file\n",
    "transcriptions = pd.read_csv('../Dataset/Transcriptions/transcriptions_complete.csv')\n",
    "# Create ID\n",
    "transcriptions[\"id\"] = transcriptions[\"clip\"].apply(extract_id)\n",
    "# Create results df\n",
    "results = transcriptions[[\"id\", 'transcription', 'confidence']]\n",
    "results = results.rename(columns = {\"confidence\": \"original_confidence\", \n",
    "                                    \"transcription\": \"original_transcription\"})\n",
    "results[\"prob_w\"] = np.nan\n",
    "results[\"unwatermarked_transcription\"] = np.nan\n",
    "results[\"unwatermarked_confidence\"] = np.nan\n",
    "\n",
    "# Create the list of ids of the remaining clips to process\n",
    "filter = results[[\"prob_w\", \"unwatermarked_transcription\", \"unwatermarked_confidence\"]].isna().any(axis = 1).values\n",
    "ids = results.loc[filter, \"id\"].values\n",
    "remaining_clips = list(set(ids) - matched.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, m in tqdm(matched.items()):\n",
    "#     un = m[0]\n",
    "#     w = m[1]\n",
    "\n",
    "#     # Load the unwatermarked audio file\n",
    "#     un_wav, sr = librosa.load(os.path.join(unwatermarked_path, un), sr = target_sr)\n",
    "#     # Convert to a PyTorch tensor\n",
    "#     un_wav_tensor = torch.tensor(un_wav).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, samples)\n",
    "\n",
    "\n",
    "#     # 1. Detect the watermark\n",
    "#     prob, _ = detector.detect_watermark(un_wav_tensor, sr)\n",
    "#     # Add the probability of watermarking to the results\n",
    "#     results.loc[results[\"id\"] == id, \"prob_w\"] = prob\n",
    "\n",
    "# # 2. Transcribe audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2**3  # Adjust based on your GPU memory\n",
    "batch_num = 0\n",
    "\n",
    "# Store results incrementally\n",
    "results_batch = []     # Stores intermediate batch results\n",
    "all_results = []       # Stores all batch DataFrames for final concat\n",
    "\n",
    "# Iterate over remaining clips in batches\n",
    "for _ in tqdm(range(0, len(remaining_clips), batch_size)):\n",
    "    batch_ids = remaining_clips[:batch_size]  # Take a batch of clip IDs\n",
    "    batch_audio = []\n",
    "    watermark_probs = []\n",
    "\n",
    "    for id_ in batch_ids:\n",
    "        un, w = matched[id_]\n",
    "        clip_path = os.path.join(unwatermarked_path, un)\n",
    "\n",
    "        # Load audio using torchaudio for Whisper compatibility\n",
    "        wav, sr = torchaudio.load(clip_path)\n",
    "\n",
    "        # Convert to mono if needed\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample to target sampling rate\n",
    "        wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n",
    "\n",
    "        # --- Step 1: Watermark detection using librosa + torch ---\n",
    "        wav_librosa, _ = librosa.load(clip_path, sr=target_sr)\n",
    "        wav_tensor = torch.tensor(wav_librosa).unsqueeze(0).unsqueeze(0)\n",
    "        prob, _ = detector.detect_watermark(wav_tensor, target_sr)\n",
    "        watermark_probs.append(float(prob))  # Ensure float type\n",
    "\n",
    "        # --- Step 2: Add audio to batch for transcription ---\n",
    "        batch_audio.append(wav)\n",
    "\n",
    "    # --- Step 3: Pad audio to equal length for batch processing ---\n",
    "    max_len = max(wav.shape[1] for wav in batch_audio)\n",
    "    batch_padded = [F.pad(wav, (0, max_len - wav.shape[1])) for wav in batch_audio]\n",
    "    batch_tensor = torch.stack(batch_padded).squeeze(1)  # (batch, time)\n",
    "\n",
    "    # --- Step 4: Convert to Whisper input format (list of numpy arrays) ---\n",
    "    batch_np = [wav.cpu().numpy() for wav in batch_tensor]\n",
    "    input_features = processor.feature_extractor(\n",
    "        batch_np, sampling_rate=target_sr, return_tensors=\"pt\"\n",
    "    ).input_features.to(device, dtype=torch.float16)\n",
    "\n",
    "    # --- Step 5: Generate transcriptions with Whisper ---\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_features,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded = processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "    # --- Step 6: Compute confidence scores ---\n",
    "    scores = outputs.scores\n",
    "    if scores:\n",
    "        token_probs = [F.softmax(logits, dim=-1).max() for logits in scores]\n",
    "        confidences = [torch.mean(p).item() for p in token_probs]\n",
    "    else:\n",
    "        confidences = [None] * len(batch_ids)\n",
    "\n",
    "    # --- Step 7: Store results in memory ---\n",
    "    for id_, transcription, confidence, prob_w in zip(batch_ids, decoded, confidences, watermark_probs):\n",
    "        results_batch.append({\n",
    "            \"id\": id_,\n",
    "            \"unwatermarked_transcription\": transcription,\n",
    "            \"unwatermarked_confidence\": confidence,\n",
    "            \"prob_w\": prob_w\n",
    "        })\n",
    "\n",
    "    # --- Step 8: Update progress ---\n",
    "    batch_num += 1\n",
    "    remaining_clips = remaining_clips[batch_size:]  # Remove processed clips\n",
    "\n",
    "    # --- Step 9: Save batch results every N batches or at the end ---\n",
    "    if (batch_num % 100 == 0) or (len(remaining_clips) == 0):\n",
    "        batch_df = pd.DataFrame(results_batch)\n",
    "        all_results.append(batch_df)\n",
    "        batch_df.to_csv(os.path.join(results_path, f\"results_batch_{batch_num // 100}.csv\"), index=False)\n",
    "        results_batch = []  # Reset batch storage\n",
    "\n",
    "# --- Step 10: Save final concatenated result (optional) ---\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "final_df.to_csv(os.path.join(results_path, \"final_results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VocalDiffusionAttack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
