{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27408"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips_path = \"../Dataset/cv-corpus-20.0-delta-2024-12-06/en/clips\"\n",
    "output_dir = \"../Dataset/Transcriptions\"\n",
    "\n",
    "clips = os.listdir(clips_path)\n",
    "clips = [i for i in clips if \".mp3\" in i[-4:]]\n",
    "len(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find clips without transcription\n",
    "transcript_files = os.listdir(output_dir)\n",
    "df = pd.DataFrame()\n",
    "for file in transcript_files:\n",
    "    if file != \"transcriptions_complete.csv\":\n",
    "        temp = pd.read_csv(os.path.join(output_dir, file))\n",
    "        df = pd.concat([df, temp], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "transcripted_clips = df[\"clip\"].unique()\n",
    "remaining_clips = list(set(clips) - set(transcripted_clips))\n",
    "len(remaining_clips)/len(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(output_dir, f\"transcriptions_complete.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [int(re.search(r\"\\d+\", i).group()) if re.search(r\"\\d+\", i) else None for i in transcript_files]\n",
    "batch_num = np.max(numbers) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device)\n",
    "\n",
    "# Whisper expects a sample rate of 16khz\n",
    "target_sr = 16000\n",
    "transcriptions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2**0  # Adjust based on your GPU memory\n",
    "transcriptions = []\n",
    "\n",
    "# for i in tqdm(range(0, len(remaining_clips), batch_size)):\n",
    "#     batch_clips = remaining_clips[i:i + batch_size]\n",
    "\n",
    "while tqdm(len(remaining_clips) > 0):\n",
    "    batch_clips = remaining_clips[:batch_size]  # Take a batch\n",
    "\n",
    "    batch_audio = []\n",
    "    for clip in batch_clips:\n",
    "        wav, sr = torchaudio.load(os.path.join(clips_path, clip))\n",
    "\n",
    "        # Convert to MONO (ensure single channel)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)  # Convert stereo -> mono\n",
    "\n",
    "        # Resample to 16kHz (required by Whisper)\n",
    "        wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n",
    "\n",
    "        batch_audio.append(wav)\n",
    "\n",
    "    # Determine max length in the batch for padding\n",
    "    max_length = max(wav.shape[1] for wav in batch_audio)\n",
    "\n",
    "    # Pad all audio clips to the same length\n",
    "    batch_audio_padded = []\n",
    "    for wav in batch_audio:\n",
    "        pad_size = max_length - wav.shape[1]\n",
    "        padded_wav = F.pad(wav, (0, pad_size))  # Pad on the right\n",
    "        batch_audio_padded.append(padded_wav)\n",
    "\n",
    "    # Convert to a single tensor and ensure correct shape for Whisper\n",
    "    batch_audio_padded = torch.stack(batch_audio_padded)  # Shape: (batch, 1, time)\n",
    "    batch_audio_padded = batch_audio_padded.squeeze(1)  # Remove channel dim -> (batch, time)\n",
    "\n",
    "    # Convert batch audio to input features (ENSURE LIST OF NUMPY ARRAYS)\n",
    "    batch_audio_numpy = [wav.cpu().numpy() for wav in batch_audio_padded]  # Whisper needs list of np.array\n",
    "    batch_input_features = processor.feature_extractor(\n",
    "        batch_audio_numpy, sampling_rate=target_sr, return_tensors=\"pt\"\n",
    "    ).input_features.to(device, dtype=torch.float16)  # Convert to FP16\n",
    "\n",
    "    # Generate transcriptions\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            batch_input_features,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "        )\n",
    "\n",
    "    # Decode results\n",
    "    batch_transcriptions = processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "    # Compute confidence scores\n",
    "    batch_log_probs = outputs.scores\n",
    "    if batch_log_probs:\n",
    "        batch_probs = [F.softmax(logits, dim=-1).max() for logits in batch_log_probs]\n",
    "        batch_avg_confidence = [torch.mean(probs).item() for probs in batch_probs]\n",
    "    else:\n",
    "        batch_avg_confidence = [None] * len(batch_clips)\n",
    "\n",
    "    transcriptions.extend(zip(batch_clips, batch_transcriptions, batch_avg_confidence))\n",
    "\n",
    "    # Remove processed clips\n",
    "    remaining_clips = remaining_clips[batch_size:]\n",
    "\n",
    "    # Save every N batches\n",
    "    if (batch_num % 50 == 0) or (len(remaining_clips) == 0 ):\n",
    "        output_file = os.path.join(output_dir, f\"transcriptions_{batch_num}.csv\")\n",
    "        df = pd.DataFrame(transcriptions, columns=[\"clip\", \"transcription\", \"confidence\"])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        transcriptions = []\n",
    "        batch_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VocalDiffusionAttack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
